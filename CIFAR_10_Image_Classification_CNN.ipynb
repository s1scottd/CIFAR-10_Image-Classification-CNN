{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQHlX5NDfQDQasIJ6m85wn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1scottd/CIFAR-10_Image-Classification-CNN/blob/main/CIFAR_10_Image_Classification_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Image Classification with CNN\n",
        "\n",
        "Convolutional Neural Network (CNN) model for image classification on the CIFAR-10 dataset.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [Technologies](#technologies)\n",
        "3. [Installation](#installation)\n",
        "4. [Data](#data)\n",
        "5. [Methodology](#methodology)\n",
        "6. [Results](#results)\n",
        "7. [Conclusion](#conclusion)\n",
        "8. [References](#references)\n",
        "\n",
        "## Introduction <a name=\"introduction\"></a>\n",
        "\n",
        "This project involves building and training a Convolutional Neural Network to classify images from the CIFAR-10 dataset.  The project is a multiclass, single label problem.\n",
        "\n",
        "## Technologies <a name=\"technologies\"></a>\n",
        "\n",
        "The project uses the following technologies and libraries:\n",
        "\n",
        "- Python: 3.10.11\n",
        "- TensorFlow: 2.12.1\n",
        "- NumPy: 1.22.4\n",
        "- Keras: 2.12.1\n",
        "\n",
        "## Installation <a name=\"installation\"></a>\n",
        "\n",
        "Follow these steps to set up the project:\n",
        "\n",
        "1. Clone the repository: `git clone https://github.com/s1scottd/CIFAR-10_classification-CNN.git`\n",
        "2. Open Google Colab and upload the notebook file `CIFAR-10_classification-CNN.ipynb` or select the \"Open in Colab\" button at the top of jupyter file.\n",
        "\n",
        "## Data <a name=\"data\"></a>\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 40000 training images, 10000 validation images and 10000 test images.\n",
        "\n",
        "## Baseline <a name=\"baseline\"><a>\n",
        "The baseline is a validation accuracy greater than 0.1. (random classifier)\n",
        "\n",
        "## Measure of Success\n",
        "This is a balanced classification problem, so the measure of success will be accuracy.\n",
        "\n",
        "## Methodology <a name=\"methodology\"></a>\n",
        "\n",
        "The project involves the following steps:\n",
        "\n",
        "1. Load and preprocess the data: The CIFAR-10 data is loaded using Keras and normalized to have pixel values between 0 and 1.\n",
        "2. Define the CNN model: A CNN model is defined using Keras with two convolutional layers, a max pooling layer, and two dense layers.\n",
        "3. Compile and train the model: The model is compiled and trained using the Adam optimizer and categorical cross-entropy loss function for 20 epochs.\n",
        "4. Evaluate the model: The model's performance is evaluated on the test data using accuracy as the metric.\n",
        "\n",
        "## Results <a name=\"results\"></a>\n",
        "\n",
        "The trained model achieves an accuracy of xx% on the test data. For a more detailed view of the model's performance and visualizations, check the Jupyter notebook `CIFAR-10_Classification.ipynb`.\n",
        "\n",
        "## Conclusion <a name=\"conclusion\"></a>\n",
        "\n",
        "This project demonstrates the effectiveness of Convolutional Neural Networks in image classification tasks. Future work might involve exploring different model architectures, or using data augmentation techniques.\n",
        "\n",
        "## References <a name=\"references\"></a>\n",
        "\n",
        "- O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others. KerasTuner, github.com/keras-team/keras-tuner. \n",
        "- Chollet, F. et al., 2015. Keras."
      ],
      "metadata": {
        "id": "s0bXL9pPqzrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries. "
      ],
      "metadata": {
        "id": "WH6tQ9ZmOePE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G66PhfVwFeAy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib as mplt\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gs\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from platform import python_version\n",
        "print(f\"python version: {python_version()}\")\n",
        "print(f\"tensorflow version: {tf.__version__}\")\n",
        "print(f\"numpy version: {np.__version__}\")\n",
        "print(f\"keras version: {keras.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##View the Raw Data##"
      ],
      "metadata": {
        "id": "c09tSZY3xCwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape} and y_train shape: {y_train.shape}\\n\")\n",
        "print(f\"X_test shape:  {X_test.shape} and y_test shape: {y_test.shape}\\n\")\n",
        "print(f\"First raw value of X_train:\\n {X_train[0]}\\n\")\n",
        "print(f\"First raw value of y_train:\\n {y_train[0]}\\n\")\n",
        "print(f\"First raw value of X_test:\\n {X_test[0]}\\n\")\n",
        "print(f\"First raw value of y_test:\\n {y_test[0]}\\n\")"
      ],
      "metadata": {
        "id": "fQDiGfTmxJ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for missing data in the CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "eOpccRwXTHCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train contains NaN: {np.any(np.isnan(X_train))}\")\n",
        "print(f\"X_test contains NaN: {np.any(np.isnan(X_test))}\")\n",
        "print(f\"y_train contains NaN: {np.any(np.isnan(y_train))}\")\n",
        "print(f\"y_test contains NaN: {np.any(np.isnan(y_test))}\\n\")"
      ],
      "metadata": {
        "id": "cXHhO-M7NEAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split X_train into a training dataset and a validation dataset."
      ],
      "metadata": {
        "id": "CooKAV_RnSEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train[:-10000]\n",
        "X_val = X_train[-10000:]\n",
        "y_train = y_train[:-10000]\n",
        "y_val = y_train[-10000:]"
      ],
      "metadata": {
        "id": "OqgtRF4XjLoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of the data"
      ],
      "metadata": {
        "id": "lIe6G1i690R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y_train, y_val and y_test into DataFrames\n",
        "y_train_df = pd.DataFrame(y_train, columns=['Classes'])\n",
        "y_val_df = pd.DataFrame(y_val, columns=['Classes'])\n",
        "y_test_df = pd.DataFrame(y_test, columns=['Classes'])\n",
        "\n",
        "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
        "\n",
        "# Count plot for training set\n",
        "sns.countplot(data=y_train_df, x='Classes', ax=axs[0])\n",
        "axs[0].set_title('Distribution of training data')\n",
        "\n",
        "# Count plot for training set\n",
        "sns.countplot(data=y_val_df, x='Classes', ax=axs[1])\n",
        "axs[1].set_title('Distribution of validation data')\n",
        "\n",
        "# Count plot for testing set\n",
        "sns.countplot(data=y_test_df, x='Classes', ax=axs[2])\n",
        "axs[2].set_title('Distribution of Testing data')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9FqBSt-m97gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define an image display function"
      ],
      "metadata": {
        "id": "uo21WLXCI5y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def display_images(data, labels):\n",
        "  \n",
        "  labels = labels.reshape(-1)\n",
        "\n",
        "  labels_dict = {\n",
        "    0: 'airplane',\n",
        "    1: 'automobile',\n",
        "    2: 'bird',\n",
        "    3: 'cat',\n",
        "    4: 'deer',\n",
        "    5: 'dog',\n",
        "    6: 'frog',\n",
        "    7: 'horse',\n",
        "    8: 'ship',\n",
        "    9: 'truck'\n",
        "  }\n",
        "\n",
        "  fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "\n",
        "  for i, ax in enumerate(axes.flat):\n",
        "    img = data[i]\n",
        "    label = labels[i]\n",
        "    \n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"{labels_dict[label]}\")\n",
        "    ax.axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "WRkHS0AfI_AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the first 9 images"
      ],
      "metadata": {
        "id": "FjzfdS8TFGCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize data by plotting images\n",
        "display_images(X_train, y_train)"
      ],
      "metadata": {
        "id": "jvUyCcyQFJ1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare the data"
      ],
      "metadata": {
        "id": "hL6X9a9BUCCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a normalize function"
      ],
      "metadata": {
        "id": "Z4mnjdPkTATZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "  data = data.astype(\"float32\")/255\n",
        "  return data"
      ],
      "metadata": {
        "id": "_lh3_T9aSrSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the dataset"
      ],
      "metadata": {
        "id": "-Wzt5yFunxek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = normalize(X_train)\n",
        "X_test = normalize(X_test)\n",
        "X_val = normalize(X_val)\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_val.shape[0], 'validation samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape:  {X_test.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n"
      ],
      "metadata": {
        "id": "STC1Slb5n3aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert class vectors to binary class matrices - one hot encoding."
      ],
      "metadata": {
        "id": "1FHfIrkX9Czx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_val = keras.utils.to_categorical(y_val, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "h2bLWHGh9O9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network without Tuning"
      ],
      "metadata": {
        "id": "8c9K-oLUORBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Model##"
      ],
      "metadata": {
        "id": "pQ2N6UHj4hEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a baseline model and compile "
      ],
      "metadata": {
        "id": "xsddREWz3Q83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_baseline_model():\n",
        "  model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',input_shape=X_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "# a softmax classifier\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "gr2JAK7Z3n0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the baseline model"
      ],
      "metadata": {
        "id": "64BuaniA4Ztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_baseline_model()\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cA4CdcWE4nam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the baseline model"
      ],
      "metadata": {
        "id": "n6uhwHpd41aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "kXSL26ZX47Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the Loss Curves"
      ],
      "metadata": {
        "id": "Uu7F68up6AWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training loss')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='validation loss')\n",
        "plt.legend(loc=\"upper right\")\n",
        "\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.figure(figsize=(3,4), dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nGb1rgbe6Bpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score trained model and predictions."
      ],
      "metadata": {
        "id": "I-CJvP0MyH39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "# make prediction.\n",
        "pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "DzmxNXbvyNwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scale up##"
      ],
      "metadata": {
        "id": "K8be2vfO5EoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and compile the CNN model"
      ],
      "metadata": {
        "id": "9eEWbdbLNwgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "\n",
        "def build_model(dropouts_enabled = False, learning_rate = 0.001):\n",
        "  \n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  if (dropouts_enabled): model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  if (dropouts_enabled): model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(128, activation=\"relu\"))\n",
        "  if (dropouts_enabled): model.add(Dropout(0.5))\n",
        "  model.add(Dense(10))\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  print(f\"Learning Rate = {learning_rate}.  Dropouts are {'enabled' if dropouts_enabled else 'not enabled'}.\\n\")\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "hOdbI6weNPg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a callback that will stop fitting the model once it reaches 95% accuracy."
      ],
      "metadata": {
        "id": "cFrQJzfRYNw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class callback_val_accuracy(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, accuracy):\n",
        "    self.accuracy = accuracy\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=[]):\n",
        "    if (logs.get(\"val_accuracy\") >= self.accuracy):\n",
        "      self.model.stop_training = True\n",
        "      print(f\"\\nReached {self.accuracy*100} validation accuracy so cancelling training after {epoch} epochs.\\n\")"
      ],
      "metadata": {
        "id": "CwvPoDR7YVcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the model"
      ],
      "metadata": {
        "id": "tBah3i0oeZ5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_model(dropouts_enabled=False, learning_rate=0.0001)"
      ],
      "metadata": {
        "id": "WOpgUogmec_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the CNN model"
      ],
      "metadata": {
        "id": "eyXC2USUNPM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "callbacks = callback_val_accuracy(0.99)\n",
        "\n",
        "#history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), callbacks=[callbacks])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=128, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "6zZLpoxrNY-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and Accuracy"
      ],
      "metadata": {
        "id": "irQS67oqb-Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training loss')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='validation loss')\n",
        "plt.legend(loc=\"upper right\")\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='validation accuracy')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.figure(figsize=(3,4), dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3XO9SF9Kf2iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "JdHMkmueNdFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "id": "at-z-j8VNdm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network with Tuning"
      ],
      "metadata": {
        "id": "y1jB8XTcPAGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import Tuner"
      ],
      "metadata": {
        "id": "FJc7hxCVHtSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade\n",
        "import keras_tuner\n",
        "keras_tuner_version = keras_tuner.__version__\n",
        "print(f\"keras_tuner version: {keras_tuner_version}\")"
      ],
      "metadata": {
        "id": "WVajH0k-H_N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tunable model"
      ],
      "metadata": {
        "id": "iTzCiyiZPgXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_model(hp):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(\n",
        "        # tune number of units\n",
        "        units = hp.Int('units', min_value=32, max_value=512, step=32),\n",
        "        # tune the activation function to use\n",
        "        activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n",
        "  model.add(Dense(10, activation=\"softmax\"))\n",
        "  \n",
        "  learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "kKtX58EIH0LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and Compile the model"
      ],
      "metadata": {
        "id": "6Gnyrd-sI_Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "build_model(keras_tuner.HyperParameters())"
      ],
      "metadata": {
        "id": "O4zmZ0T_Q7HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tuner"
      ],
      "metadata": {
        "id": "WBavstE7Q8Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=100,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name = \"cifar10-image-classification-cnn\")"
      ],
      "metadata": {
        "id": "C5FSHGSsI_fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute a search for the best model"
      ],
      "metadata": {
        "id": "CQOlv45vJSkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early =  callback_val_accuracy(0.995)\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[stop_early])\n",
        "best_model = tuner.get_best_models()[0]\n",
        "print(f\"Search Space Summary:\\n{tuner.search_space_summary}\")"
      ],
      "metadata": {
        "id": "VmgYXH0PJTLq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}